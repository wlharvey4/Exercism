# -*- mode:org; fill-column:79; -*-
#+Title:Exercism Exercises
#+Author:Pinecone062
#+Date:Last updated 2019-05-20 21:15
#+Macro: version Version 0.1.3
{{{title}}} {{{version}}} {{{date}}}
* Exercism Home
:PROPERTIES:
:unnumbered: t
:END:
- https://github.com/exercism

** Exercism Readme

#+name:Exercism-Readme
#+BEGIN_SRC txt :tangle README.md
  # Welcome to Exercism

  This is the central point of contact for Exercism.

  ## Where to open issues

  The project is divided across many repositories.

  #### The website or product

  If you have any issues or questions regarding **the website
  (https://exercism.io)** please [open an issue in this
  repository](https://github.com/exercism/exercism/issues). To report
  errors in the website copy or submit fixes for typos or other
  improvements, please see the
  [exercism/website-copy](https://github.com/exercism/website-copy/issues)
  repository.

  #### The Command-Line Client (CLI)

  For problems with the **Command-Line Client (CLI)**, open an issue in
  [exercism/cli](https://github.com/exercism/cli/issues).

  #### Exercises in a particular programming language

  The exercises are all in separate, language-specific
  repositories. These repositories are tagged with the topic
  [`#exercism-track`](https://github.com/search?q=topic%3Aexercism-track+org%3Aexercism&type=Repositories). From
  there, search for the track (programming language) you are
  participating in.

  ,**Unsure? Open your issue here**

  If you aren't sure where to open it, then pick this repository. It's
  as good a starting point as any!

  ## Feeling uncomfortable?

  If you need to report a code of conduct violation, please email us at team@exercism.io.

  ## Where to find the code

  The code for the website lives in [exercism/website](http://github.com/exercism/website).

  The code for the old website is in this repository, in the
  [v1.exercism.io](https://github.com/exercism/exercism/tree/v1.exercism.io)
  branch.

  ## Who's behind Exercism?

  Read about our Team on the site: https://exercism.io/team
#+end_src

** Exercism CLI
- https://github.com/exercism/cli

To install manually, download the appropriate version of exercism based on your
processor architecture at the [[https://github.com/exercism/cli/releases/latest][releases page]], then extract the archive and put
the binary in your path.

Check the installed version with:

#+BEGIN_SRC sh :results output :exports both
exercism version
#+END_SRC

#+RESULTS:
: exercism version 3.0.11

In order to configure the CLI, paste in the following text into your terminal:
: exercism configure --token=b4b5ab41-b448-4762-a5ce-452ccd2eff08

 You should see a notification from the CLI that a configuration file has been
 written.

#+begin_src txt
You have configured the Exercism command-line client:

Config dir:                       ~/.config/exercism
Token:         (-t, --token)      b4b5ab41-b448-4762-a5ce-452ccd2eff08
Workspace:     (-w, --workspace)  <path-to>/Exercism
API Base URL:  (-a, --api)        https://api.exercism.io/v1
#+end_src

** Exercism Tangle
This code extracts all of the Exercism files to disk upon Export.
#+name:exercism-tangle
#+BEGIN_SRC emacs-lisp :results output :exports both
(org-babel-tangle-file "Exercism.org")
#+END_SRC
* Python Exercism Track
- http://exercism.io/languages/python
- https://github.com/exercism/python

The Python code in this repo is meant to follow the [[https://www.python.org/dev/peps/pep-0008/][PEP8 style guide]] (a
stylized version http://pep8.org).

This repo uses [[http://flake8.readthedocs.org/en/latest/][flake8]] with default settings to enforce the coding standard.

** Python Exercism README
#+name:Readme-Python-track
#+begin_src txt :tangle python/README.md :mkdirp yes
# Exercism Python Track

[![Build Status](https://travis-ci.org/exercism/python.svg?branch=master)](https://travis-ci.org/exercism/python) [![Requirements Status](https://pyup.io/repos/github/exercism/python/shield.svg)](https://pyup.io/repos/github/exercism/python/)
[![Join the chat at https://gitter.im/exercism/python](https://badges.gitter.im/exercism/python.svg)](https://gitter.im/exercism/python?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge&utm_content=badge)

Exercism exercises in Python


## Contributing Guide

Please see the [contributing guide](https://github.com/exercism/docs/blob/master/contributing-to-language-tracks/README.md)


## Working on the Exercises

We welcome both improvements to the existing exercises and new exercises.
A list of missing exercise can be found here: https://github.com/exercism/python/issues/417#issuecomment-366040062


### Conventions

- We use minimalistic stub files for all exercises ([#272](https://github.com/exercism/python/issues/272)).
- We use `unittest` (Python Standard Library) and no 3rd-party-framework.
- We use the parameter order `self.assertEqual(actual, expected)` ([#440](https://github.com/exercism/python/issues/440)).
- We use context managers (`with self.assertRaises(\<exception type\>):`) for testing for exceptions ([#477](https://github.com/exercism/python/issues/477)).
- We use an established utility method to confirm that expected exceptions contain a non-empty message. This method must be included for any test class with an exception-based test case ([#1080](https://github.com/exercism/python/issues/1080#issuecomment-442068539)).
- We use `assertIs(actual, True)` and `assertIs(actual, False)` rather than `assertTrue(actual)` or `assertFalse(actual)` ([#419](https://github.com/exercism/python/pull/419)).
- We use a comment string in the test file to reference the version of the exercise's `canonical-data.json` that tests were adapted from (wording can be found in: [#784](https://github.com/exercism/python/issues/784)).


### Testing

All exercises must be compatible with Python versions 2.7 and 3.4 upwards.

To test a single exercise (e.g., with Python 2.7):
```
python2.7 test/check-exercises.py [exercise-name]
```

To test all exercises (e.g., with Python 3):
```
python3 test/check-exercises.py
```


### Code Style

The Python code in this repo is meant to follow the [PEP8 style guide](https://www.python.org/dev/peps/pep-0008/) (a stylized version http://pep8.org).

This repo uses [flake8](http://flake8.readthedocs.org/en/latest/) with default settings to enforce the coding standard.


### CI build

This repo uses `travis-ci` in the following configuration: [travis.yml](https://github.com/exercism/python/blob/master/.travis.yml)

It will automatically check the code style, the problem configuration, and run the unittests with all supported Python versions.


## Pull Requests

We :heart: pull requests!
We even :sparkling_heart: them if they contain well written commit messages!

Please write the first line of your commit message in the following style:

```exercise-name: Change some things```

Please try to follow the [The seven rules of a great Git commit message](https://chris.beams.io/posts/git-commit/#seven-rules) like to capitalize the subject line and use the imperative mood. If there are more details to add, put those into the body of the commit message.

If you're interested, Tim Pope even has an [entire blog post](http://tbaggery.com/2008/04/19/a-note-about-git-commit-messages.html) on good commit messages.

If you're new to Git, take a look at [this short guide](https://github.com/exercism/docs/blob/master/contributing-to-language-tracks/README.md#git-basics).

## License
This repository uses the [MIT License](/LICENSE).
#+end_src

** Python Hello World
Write a function that returns the string "Hello, World!"

You can tell Python to run the ~pytest~ module (allowing the same command to be
used regardless of Python version):

: python -m pytest hello_world_test.py

Changed computer or want to re-download the exercise after updating to the
latest version? Use this command:

: exercism download --exercise=hello-world --track=python

: Downloaded to
: /usr/local/dev/programming/Exercism/python/hello-world

*** Python Hello World Solution

#+name:hello_world.py
#+begin_src python -n :tangle python/hello-world/hello_world.py :mkdirp yes
def hello():
    return "Hello, World!"
#+end_src

#+name:python-hello-world-metadata
#+BEGIN_SRC js :tangle python/hello-world/.exercism/metadata.json :mkdirp yes :exports none
{
    "track":"python",
    "exercise":"hello-world",
    "id":"86a587c4d6e34a078741d6be1edce608",
    "url":"https://exercism.io/my/solutions/86a587c4d6e34a078741d6be1edce608",
    "handle":"wlharvey4",
    "is_requester":true,
    "auto_approve":true
}
#+END_SRC

*** Python Hello World Test

#+name:Python-Hello-World-Test
#+begin_src python :tangle python/hello-world/hello_world_test.py :mkdirp yes
import unittest

import hello_world

# Tests adapted from `problem-specifications//canonical-data.json` @ v1.1.0

class HelloWorldTest(unittest.TestCase):
    def test_hello(self):
        self.assertEqual(hello_world.hello(), 'Hello, World!')


if __name__ == '__main__':
    unittest.main()
#+end_src

#+name:python-hello-world-test
#+begin_src sh :dir python/hello-world :results output :exports both
python3 -m pytest hello_world_test.py
#+end_src

#+RESULTS: python-hello-world-test
: ============================= test session starts ==============================
: platform darwin -- Python 3.7.3, pytest-4.4.1, py-1.8.0, pluggy-0.9.0
: rootdir: /usr/local/dev/programming/Languages/Exercism/python/hello-world
: collected 1 item
: 
: hello_world_test.py .                                                    [100%]
: 
: =========================== 1 passed in 0.10 seconds ===========================

** Python Two Fer
Two-fer or 2-fer is short for two for one. One for you and one for me.

: "One for X, one for me."

When X is a name or "you".

If the given name is "Alice", the result should be "One for Alice, one for me."
If no name is given, the result should be "One for you, one for me."

To run the tests:
: python3 -m pytest two_fer_test.py

*** Python Two Fer Solution
#+name:Two-Fer-Solution
#+begin_src python -n :tangle python/two-fer/two_fer.py :mkdirp yes
def two_fer(name="you"):
    return f"One for {name}, one for me."
#+end_src

#+begin_src sh :dir python/two-fer :results output
python3 -m pytest two_fer_test.py
#+end_src

#+RESULTS:
: ============================= test session starts ==============================
: platform darwin -- Python 3.7.3, pytest-4.4.1, py-1.8.0, pluggy-0.9.0
: rootdir: /usr/local/dev/programming/Languages/Exercism/python/two-fer
: collected 3 items
: 
: two_fer_test.py ...                                                      [100%]
: 
: =========================== 3 passed in 0.02 seconds ===========================

#+name:python-two-fer-metadata
#+BEGIN_SRC js :tangle python/two-fer/.exercism/metadata.json :mkdirp yes :exports none
{
    "track":"python",
    "exercise":"two-fer",
    "id":"80dbb050e4f041efa796149983871746",
    "url":"https://exercism.io/my/solutions/80dbb050e4f041efa796149983871746",
    "handle":"wlharvey4",
    "is_requester":true,
    "auto_approve":false
}
#+END_SRC

*** Python Two Fer Test
#+name:Python-Two-Fer-Test
#+begin_src python :tangle python/two-fer/two_fer_test.py :mkdirp yes
import unittest

from two_fer import two_fer


# Tests adapted from `problem-specifications//canonical-data.json` @ v1.2.0

class TwoFerTest(unittest.TestCase):
    def test_no_name_given(self):
        self.assertEqual(two_fer(), 'One for you, one for me.')

    def test_a_name_given(self):
        self.assertEqual(two_fer("Alice"), "One for Alice, one for me.")

    def test_another_name_given(self):
        self.assertEqual(two_fer("Bob"), "One for Bob, one for me.")


if __name__ == '__main__':
    unittest.main()
#+end_src

#+begin_src sh :dir python/two-fer :results output :exports both
python3 -m pytest two_fer_test.py
#+end_src

#+RESULTS:
: ============================= test session starts ==============================
: platform darwin -- Python 3.7.3, pytest-4.4.1, py-1.8.0, pluggy-0.9.0
: rootdir: /usr/local/dev/programming/Languages/Exercism/python/two-fer
: collected 3 items
: 
: two_fer_test.py ...                                                      [100%]
: 
: =========================== 3 passed in 0.06 seconds ===========================

** High Scores
:PROPERTIES:
:level:    easy
:END:

*** Task for High Scores
Manage a game player's High Score list.

Your task is to build a high-score component of the classic Frogger game, one
of the highest selling and addictive games of all time, and a classic of the
arcade era.  Your task is to write methods that return the highest score from
the list, the last added score, the three highest scores, and a report on the
difference between the last and the highest scores.

*** Readme for High Scores

#+name:High_Scores_Python_Readme
#+begin_src txt :tangle python/high-scores/README.md :mkdirp yes
  # High Scores

  Manage a game player's High Score list.

  Your task is to build a high-score component of the classic Frogger
  game, one of the highest selling and addictive games of all time, and
  a classic of the arcade era. Your task is to write methods that return
  the highest score from the list, the last added score, the three
  highest scores, and a report on the difference between the last and
  the highest scores.

  ## Submitting Exercises

  Note that, when trying to submit an exercise, make sure the solution
  is in the `exercism/python/<exerciseName>` directory.

  For example, if you're submitting `bob.py` for the Bob exercise, the
  submit command would be something like `exercism submit
  <path_to_exercism_dir>/python/bob/bob.py`.


  For more detailed information about running tests, code style and linting,
  please see the [help page](http://exercism.io/languages/python).

  ## Source

  Tribute to the eighties' arcade game Frogger

  ## Submitting Incomplete Solutions
  It's possible to submit an incomplete solution so you can see how others have completed the exercise.

#+end_src

*** Python High Scores Code
#+name:High_Scores_Python
#+begin_src python -n :tangle python/high-scores/high_scores.py :mkdirp yes
class HighScores(object):
    def __init__(self, scores):
        self.scores = scores

    def latest(self):
        return self.scores[-1]

    def personal_best(self):
        best = 0
        for score in self.scores:
            if score > best:
                best = score

        return best

    def personal_top(self):
        pt = self.scores[:]
        pt.sort(reverse=True)
        return pt[:3]

    def report(self):
        latest = self.scores[-1]
        best = self.personal_best()
        report = f"Your latest score was {latest}. "
        if latest == best:
            report += f"That's your personal best!"
        else:
            report += f"That's {best - latest} short of your personal best!"

        return report
#+end_src

#+name:python-high-scores-metadata
#+BEGIN_SRC js :tangle python/high-scores/.exercism/metadata.json :mkdirp yes :exports none
{
    "track":"python",
    "exercise":"high-scores",
    "id":"485c09d00b054d51b421c3d06a7cec30",
    "url":"https://exercism.io/my/solutions/485c09d00b054d51b421c3d06a7cec30",
    "handle":"wlharvey4",
    "is_requester":true,
    "auto_approve":false
p}
#+END_SRC

*** Python High Scores Test
#+name:High_Scores_Python_Test
#+begin_src python :tangle python/high-scores/high_scores_test.py :mkdirp yes
import unittest

from high_scores import HighScores


# Tests adapted from `problem-specifications//canonical-data.json` @ v2.0.0


class HighScoreTest(unittest.TestCase):
    def test_list_of_scores(self):
        scores = [30, 50, 20, 70]
        expected = [30, 50, 20, 70]
        self.assertEqual(HighScores(scores).scores, expected)

    def test_latest_score(self):
        scores = [100, 0, 90, 30]
        expected = 30
        self.assertEqual(HighScores(scores).latest(), expected)

    def test_personal_best(self):
        scores = [40, 100, 70]
        expected = 100
        self.assertEqual(HighScores(scores).personal_best(), expected)

    def test_personal_top(self):
        scores = [50, 30, 10]
        expected = [50, 30, 10]
        self.assertEqual(HighScores(scores).personal_top(), expected)

    def test_personal_top_highest_to_lowest(self):
        scores = [20, 10, 30]
        expected = [30, 20, 10]
        self.assertEqual(HighScores(scores).personal_top(), expected)

    def test_personal_top_when_there_is_a_tie(self):
        scores = [40, 20, 40, 30]
        expected = [40, 40, 30]
        self.assertEqual(HighScores(scores).personal_top(), expected)

    def test_personal_top_when_there_are_less_than_3(self):
        scores = [30, 70]
        expected = [70, 30]
        self.assertEqual(HighScores(scores).personal_top(), expected)

    def test_personal_top_when_there_is_only_one(self):
        scores = [40]
        expected = [40]
        self.assertEqual(HighScores(scores).personal_top(), expected)

    def test_personal_top_from_a_long_list(self):
        scores = [10, 30, 90, 30, 100, 20, 10, 0, 30, 40, 40, 70, 70]
        expected = [100, 90, 70]
        self.assertEqual(HighScores(scores).personal_top(), expected)

    def test_message_for_new_personal_best(self):
        scores = [20, 40, 0, 30, 70]
        expected = "Your latest score was 70. That's your personal best!"
        self.assertEqual(HighScores(scores).report(), expected)

    def test_message_when_latest_score_is_not_the_highest_score(self):
        scores = [20, 100, 0, 30, 70]
        expected = (
            "Your latest score was 70. That's 30 short of your personal best!"
        )
        self.assertEqual(HighScores(scores).report(), expected)

    def test_message_for_repeated_personal_best(self):
        scores = [20, 70, 50, 70, 30]
        expected = (
            "Your latest score was 30. That's 40 short of your personal best!"
        )
        self.assertEqual(HighScores(scores).report(), expected)


if __name__ == "__main__":
    unittest.main()
#+end_src

#+name:Test_High_Score_Python
#+begin_src sh :dir python/high-scores :results output :exports both
python3 -m pytest high_scores_test.py
#+end_src

#+RESULTS: Test_High_Score_Python
: ============================= test session starts ==============================
: platform darwin -- Python 3.7.3, pytest-4.4.1, py-1.8.0, pluggy-0.9.0
: rootdir: /usr/local/dev/programming/Languages/Exercism/python/high-scores
: collected 12 items
: 
: high_scores_test.py ............                                         [100%]
: 
: ========================== 12 passed in 0.08 seconds ===========================

* Perl5 Exercism Track
** Perl5 Hello World
The classical introductory exercise. Just say "Hello, World!".

"Hello, World!" (https://en.wikipedia.org/wiki/%22Hello%2c_World!%22_program)
is the traditional first program for beginning programming in a new language or
environment.

{{{heading(Objectives)}}}

The objectives are simple:
- Write a function that returns the string "Hello, World!".
- Run the test suite and make sure that it succeeds.
- Submit your solution and check it at the website.


If everything goes well, you will be ready to fetch your first real exercise.

*** Perl5 Hello World Readme
#+name:perl5-hello-world-readme
#+BEGIN_SRC txt :tangle perl5/README.md :mkdirp yes
# Hello World

The classical introductory exercise. Just say "Hello, World!".

["Hello, World!"](http://en.wikipedia.org/wiki/%22Hello,_world!%22_program) is
the traditional first program for beginning programming in a new language
or environment.

The objectives are simple:

- Write a function that returns the string "Hello, World!".
- Run the test suite and make sure that it succeeds.
- Submit your solution and check it at the website.

If everything goes well, you will be ready to fetch your first real exercise.
## Source

This is an exercise to introduce users to using Exercism
[http://en.wikipedia.org/wiki/%22Hello,_world!%22_program](http://en.wikipedia.org/wiki/%22Hello,_world!%22_program)

## Submitting Incomplete Solutions
It's possible to submit an incomplete solution so you can see how others have completed the exercise.
#+END_SRC

*** Perl5 Hello World Solution
#+name:perl5-hello-world-code
#+BEGIN_SRC perl -n :tangle perl5/hello-world/lib/HelloWorld.pm :mkdirp yes
# Declare package 'HelloWorld'
package HelloWorld;
use strict;
use warnings;
use Exporter 'import';
our @EXPORT_OK = qw(hello);

sub hello {
  return "Hello, World!";
}

1;
#+END_SRC

#+name:perl5-hello-world-metadata
#+header: :exports none
#+BEGIN_SRC js :tangle perl5/hello-world/.exercism/metadata.json :mkdirp yes :exports none
{
    "track":"perl5",
    "exercise":"hello-world",
    "id":"55fe9523b94f4e1b9a0b7f8a438d0e45",
    "url":"https://exercism.io/my/solutions/55fe9523b94f4e1b9a0b7f8a438d0e45",
    "handle":"wlharvey4",
    "is_requester":true,
    "auto_approve":true
}
#+END_SRC

*** Perl5 Hello World Test
#+name:perl5-hello-world-test
#+BEGIN_SRC perl -n :tangle perl5/hello-world/t/hello-world.t :mkdirp yes
#!/usr/bin/env perl
use strict;
use warnings;
use Test::More tests => 2; # This is how many tests we expect to run.
use lib './lib';
use HelloWorld qw(hello);

can_ok 'HelloWorld', 'import' or BAIL_OUT 'Cannot import subroutines from module';

# Run the 'is' subroutine from the 'Test::More' module, with three arguments.
is(
  hello,           # Run the 'hello' subroutine, which is imported from your module.
  'Hello, World!', # The expected result to compare with 'hello'.
  'Say Hi!'        # The test description.
);
#+END_SRC

#+name:perl5-hello-world-run-test
#+BEGIN_SRC sh :dir perl5/hello-world :results output :exports both
prove ./t
#+END_SRC

#+RESULTS: perl5-hello-world-run-test
: t/hello-world.t .. ok
: All tests successful.
: Files=1, Tests=2,  0 wallclock secs ( 0.02 usr  0.00 sys +  0.03 cusr  0.00 csys =  0.05 CPU)
: Result: PASS
** Perl5 Two Fer
"Two-fer" or "2-fer" is short for "two for one.  One for you and one for me."

{{{heading(Objective)}}}

Given a name, return a string with the message:
: One for X, one for me.
Where 'X' is the given name.

However, if the name is missing, return the string:
: One for you, one for me.

*** Perl5 Two Fer Readme
#+name:perl5-two-fer-readme
#+BEGIN_SRC txt :tangle perl5/two-fer/README.md :mkdirp yes
# Two Fer

`Two-fer` or `2-fer` is short for two for one. One for you and one for me.

Given a name, return a string with the message:

```text
One for X, one for me.
```

Where X is the given name.

However, if the name is missing, return the string:

```text
One for you, one for me.
```

Here are some examples:

|Name    | String to return 
|:------:|:-----------------: 
|Alice   | One for Alice, one for me. 
|Bob     | One for Bob, one for me.
|        | One for you, one for me.
|Zaphod  | One for Zaphod, one for me.
## Source

[https://github.com/exercism/problem-specifications/issues/757](https://github.com/exercism/problem-specifications/issues/757)

## Submitting Incomplete Solutions
It's possible to submit an incomplete solution so you can see how others have completed the exercise.
#+END_SRC
*** Perl5 Two Fer Solution

#+CINDEX:defined-or operator (@code{//})
#+CINDEX:@code{//}, defined-or operator
The defined-or operator, =//=, tests the definedness of its operand.  Unlike
=||=, which tests the truth of its operand, =//= evaluates to a =true= value
even if its operand evaluates to a numeric zero or the empty string.  This is
especially useful for setting default parameter values.

#+name:perl5-two-fer-solution
#+BEGIN_SRC perl -n :tangle perl5/two-fer/lib/TwoFer.pm :mkdirp yes
package TwoFer;
use strict;
use warnings;
use Exporter 'import';
our @EXPORT_OK = qw(two_fer);

sub two_fer {
  return "One for ". eval {pop // "you"}. ", one for me.";
}

1;
#+END_SRC

#+name:perl5-two-fer-metadata
#+header: :exports none
#+BEGIN_SRC js :tangle perl5/two-fer/.exercism/metadata.json :mkdirp yes :exports none
{
    "track":"perl5",
    "exercise":"two-fer",
    "id":"4acecc77f2324da5aa5c59e52c0c0a6a",
    "url":"https://exercism.io/my/solutions/4acecc77f2324da5aa5c59e52c0c0a6a",
    "handle":"wlharvey4",
    "is_requester":true,
    "auto_approve":false
}
#+END_SRC

*** Perl5 Two Fer Test
#+perl5-two-fer-test
#+BEGIN_SRC perl -n :tangle perl5/two-fer/t/two-fer.t :mkdirp yes
#!/usr/bin/env perl
use strict;
use warnings;
use Test::More tests => 4;
use JSON::PP;
use lib './lib';
use TwoFer qw(two_fer);

can_ok 'TwoFer', 'import' or BAIL_OUT 'Cannot import subroutines from module';

my $C_DATA = do { local $/; decode_json(<DATA>); };
foreach my $case (@{$C_DATA->{cases}}) {
  is two_fer($case->{input}{name}), $case->{expected}, $case->{description};
}

__DATA__
{
  "exercise": "two-fer",
  "version": "1.2.0",
  "cases": [
    {
      "description": "no name given",
      "property": "twoFer",
      "input": {
        "name": null
      },
      "expected": "One for you, one for me."
    },
    {
      "description": "a name given",
      "property": "twoFer",
      "input": {
        "name": "Alice"
      },
      "expected": "One for Alice, one for me."
    },
    {
      "description": "another name given",
      "property": "twoFer",
      "input": {
        "name": "Bob"
      },
      "expected": "One for Bob, one for me."
    }
  ]
}
#+END_SRC

#+name:perl5-two-fer-run-test
#+BEGIN_SRC sh :dir perl5/two-fer :results output :exports both
prove ./t
#+END_SRC

#+RESULTS: perl5-two-fer-run-test
: t/two-fer.t .. ok
: All tests successful.
: Files=1, Tests=4,  0 wallclock secs ( 0.01 usr  0.01 sys +  0.05 cusr  0.00 csys =  0.07 CPU)
: Result: PASS

* Exercism Makefile
*Note*: Make sure to export the environment variable
 @@texinfo:@env{PROGRAMMING}@@.
#+name:Exercism-Makefile
#+begin_src makefile :tangle Makefile
BASE_DIR := $(PROGRAMMING)/Languages/Exercism
LANGUAGES = python perl5
HTML_SRC = *.html
TEXINFO_SRC = *.{texi,info,pdf}
DVI_SRC = *.{aux,log,toc}

.phony : texinfo-clean
texinfo-clean :
	cd $(BASE_DIR) && rm -vrf $(TEXINFO_SRC)

.phony : html-clean
html-clean :
	cd $(BASE_DIR) && rm -vrf $(HTML_SRC)

.phony : dist-clean
dist-clean :
	cd $(BASE_DIR) && rm -vrf $(LANGUAGES) README.md $(DVI_SRC)

.phony : world-clean
world-clean : dist-clean html-clean texinfo-clean
	rm Makefile

#+end_src

* Export Settings                                                  :noexport:
** HTML Export
#+options: html-link-use-abs-url:nil html-postamble:auto html-preamble:t
#+options: html-scripts:t html-style:t html5-fancy:t tex:t
#+html_doctype: html5
#+html_container: div
#+description:Exercises from Exercism.io Python language
#+keywords:exercism.io exercises Python
#+html_link_home:
#+html_link_up:
#+html_mathjax:
#+html_head:
#+html_head_extra:
#+infojs_opt:
#+creator: <a href="https://www.gnu.org/software/emacs/">Emacs</a> 26.1 (<a href="https://orgmode.org">Org</a> mode 9.2.2)
#+latex_header:
** TEXINFO Export                                                 :noexport:
#+TEXINFO_FILENAME:Exercism.info
#+TEXINFO_CLASS: info
#+TEXINFO_HEADER:
#+TEXINFO_POST_HEADER:
#+TEXINFO_DIR_CATEGORY:Languages
#+TEXINFO_DIR_TITLE:Exercism
#+TEXINFO_DIR_DESC:Learning new languages through exercises
#+TEXINFO_PRINTED_TITLE:Exercism Exercises
** MACROS                                                         :noexport:
#+macro: heading @@texinfo:@heading @@$1
#+macro: subheading @@texinfo:@subheading @@$1
* Local Variables                                                  :noexport:
# Local Variables:
# time-stamp-pattern:"8/^\\#\\+[dD]ate:Last updated %:y-%02m-%02d %02H:%02M$"
# End:
